# Local LLM Browser Extension

A Chrome extension that integrates browsers with locally hosted Large Language Models (LLMs). This extension leverages the context of the webpage you're currently viewing to power AI-driven insights, summaries, or custom prompts, while keeping data local.

![overview.png](assets/overview.png)

## Features

-   **Page Context Extraction**: Automatically extracts the text content of the active tab to provide context to your LLM.
-   **Local LLM Integration**: Connects to local services (like [Ollama](https://ollama.com/)) via customizable ports and model names.
-   **Customizable Prompts**: Define a global system prompt to tailor how the AI interacts with data.
-   **Rich Preview**: Supports Markdown and diagram rendering with `marked`, `mermaid.js` and `DOMPurify` for a clean, safe preview of custom instructions.
-   **Privacy Focused**: Settings and page data stay local. No external API keys are required for the connection to a local instance.

## Tech Stack

-   **Frontend**: [React 19](https://react.dev/) with [Material UI (MUI)](https://mui.com/)
-   **Language**: [TypeScript](https://www.typescriptlang.org/)
-   **Build Tool**: [Webpack 5](https://webpack.js.org/) with Babel and TS-Loader
-   **Security**: [DOMPurify](https://github.com/cure53/dompurify) for sanitizing rendered Markdown

## Prerequisites

- [Node.js](https://nodejs.org/) (latest LTS recommended)
- A Chromium-based browser (Chrome, Edge, Brave, etc.)
- A local LLM runner (e.g., **Ollama**) running on your machine.
  - CORS must be configured to allow requests from the extension, e.g., `OLLAMA_ORIGINS='chrome://extensions/*' ollama serve`

## Installation & Setup

### 1. Build the Extension
First, install the dependencies and build the project:

```bash
# Install dependencies
npm install

# Build the extension for production
npm run build

# Or run in watch mode for development
npm run watch
```
### 2. Load the Extension in Chrome
-    Open Chrome and navigate to chrome://extensions/.
-    Enable Developer mode (toggle in the top right).
-    Click Load unpacked.
-    Select the dist folder generated by the build process.

### 3. Configuration
- Once installed, right-click the extension icon and select Options (or use the Options page) to configure your setup:
  - Model Name: The identifier for your local model (e.g., llama3, mistral).
  - Port: The port your local LLM service is listening on (default is usually 11434 for Ollama).
  - Additional Context: Provide high-level instructions or personas (e.g., "Summarize every page in 3 bullet points").

## Project Structure
- **chrome/:** Contains background scripts and content scripts.
- **content.ts:** Handles communication with the webpage and extracts text.
- **react/:** Contains the React source code for the UI components.
- **options.tsx:** The configuration dashboard.
- **html/:** HTML entry points for the extension popups and options.
- **manifest.json:** The extension manifest defining permissions and entry points.
- **webpack.config.cjs:** Configuration for bundling the TypeScript and React code.